{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcfa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "input_width = 640 #@param {type:\"slider\", min:32, max:4096, step:32}\n",
    "input_height = 640 #@param {type:\"slider\", min:32, max:4096, step:32}\n",
    "optimize_cpu = False\n",
    "\n",
    "model = YOLO(f\"/Users/eben/Desktop/sdsmt/Projects/Tello/best_yolov8n.pt\") \n",
    "model.export(format=\"onnx\", imgsz=[input_height,input_width], optimize=optimize_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635ee59",
   "metadata": {},
   "source": [
    "# Export YOLOv8 Model for Jetson Nano (ONNX â†’ TensorRT) using Google Colab\n",
    "This notebook demonstrates how to export a YOLOv8 PyTorch model to ONNX format using Google Colab, and provides instructions for converting ONNX to TensorRT engine on Jetson Nano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995122da",
   "metadata": {},
   "source": [
    "## Next Steps (on Jetson Nano)\n",
    "1. Download the exported `.onnx` file from Colab to your local machine, then copy it to your Jetson Nano.\n",
    "2. On the Jetson Nano, use Ultralytics or the TensorRT tools to convert the ONNX model to a TensorRT engine:\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "model = YOLO('best_yolov8n.onnx')\n",
    "model.export(format='engine', imgsz=640, device='cuda')\n",
    "```\n",
    "Or use the command line:\n",
    "```sh\n",
    "yolo export model=best_yolov8n.onnx format=engine device=cuda imgsz=640\n",
    "```\n",
    "\n",
    "This will generate a `.engine` file optimized for the Jetson Nano."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b10440",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "- Export ONNX in Google Colab (or on your Mac).\n",
    "- Convert ONNX to TensorRT engine on Jetson Nano.\n",
    "- Use the `.engine` file for fast inference on Jetson Nano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export YOLOv8 Model to ONNX and TensorRT (Google Colab Workflow)\n",
    "This notebook demonstrates how to export a YOLOv8 PyTorch model to ONNX format and then to TensorRT engine using Google Colab. This workflow is ideal if you do not have a local NVIDIA GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Ultralytics and TensorRT dependencies (Colab only)\n",
    "!pip install ultralytics --upgrade\n",
    "!pip install onnx onnxruntime\n",
    "# For TensorRT export, you may need to install the TensorRT Python API if not already available in Colab\n",
    "# !pip install tensorrt  # Uncomment if needed and available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Upload your YOLOv8 .pt model to Colab (or mount Google Drive)\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # Or use Google Drive mounting if preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Export to ONNX format\n",
    "from ultralytics import YOLO\n",
    "\n",
    "input_width = 640\n",
    "input_height = 640\n",
    "optimize_cpu = False\n",
    "\n",
    "model = YOLO('best_yolov8n.pt')  # Use the uploaded or Drive path\n",
    "model.export(format='onnx', imgsz=[input_height, input_width], optimize=optimize_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. (Optional) Download the ONNX file to your local machine\n",
    "from google.colab import files\n",
    "files.download('best_yolov8n.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Export to TensorRT engine (if Colab runtime supports it)\n",
    "# Note: This step requires a Colab GPU runtime with TensorRT support. If not available, do this step on Jetson Nano.\n",
    "model = YOLO('best_yolov8n.onnx')\n",
    "model.export(format='engine', imgsz=640, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. (Optional) Download the TensorRT engine file to your local machine\n",
    "from google.colab import files\n",
    "files.download('best_yolov8n.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- If Colab does not support TensorRT export (step 5), download the ONNX file and convert it to TensorRT on your Jetson Nano as described previously.\n",
    "- Always verify the exported engine on your target device (Jetson Nano) for compatibility and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tello",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
